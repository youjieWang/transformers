{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer,会根据你传入的词是什么样子的来分类传递到需要的选择的模型上\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace服务器上加载，如果本地有，就从本地上加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer\\\\tokenizer_config.json',\n",
       " './roberta_tokenizer\\\\special_tokens_map.json',\n",
       " './roberta_tokenizer\\\\vocab.txt',\n",
       " './roberta_tokenizer\\\\added_tokens.json',\n",
       " './roberta_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以手动指定tokenizer保存在本地的位置\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果已经保存到本地了，那我们from_pretrained就可以选择从本地加载，而不会再去远程HuggingFace下载模型了\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer\")\n",
    "tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens  # 这个句子分词是不固定，知识说我们当前选择的这个分词器模型会将我们的句子拆分成这个样子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##饕': 20699,\n",
       " '饶': 7657,\n",
       " '##龕': 21043,\n",
       " '蒐': 5883,\n",
       " '姬': 2010,\n",
       " 'r9': 12674,\n",
       " '鲈': 7827,\n",
       " '子': 2094,\n",
       " 'hadoop': 10956,\n",
       " '##ge': 8441,\n",
       " '##囧': 14790,\n",
       " 'dan': 11404,\n",
       " '##捨': 16000,\n",
       " '##菽': 18897,\n",
       " '蓝': 5905,\n",
       " '訝': 6252,\n",
       " '##瘓': 17656,\n",
       " '祈': 4857,\n",
       " '##群': 18465,\n",
       " '##nel': 12827,\n",
       " '亚': 762,\n",
       " '##圭': 14821,\n",
       " '##眦': 17755,\n",
       " '2756': 9397,\n",
       " 'nas': 10157,\n",
       " '458': 12562,\n",
       " 'bean': 13188,\n",
       " '企': 821,\n",
       " '##饍': 20696,\n",
       " '咧': 1485,\n",
       " 'library': 11616,\n",
       " '耻': 5459,\n",
       " '3s': 13247,\n",
       " '##嬌': 15138,\n",
       " '##尘': 15269,\n",
       " '糜': 5134,\n",
       " '##泡': 16853,\n",
       " '綾': 5213,\n",
       " '罹': 5395,\n",
       " 'ч': 255,\n",
       " '##谤': 19527,\n",
       " '睡': 4717,\n",
       " '粪': 5116,\n",
       " 'nike': 8702,\n",
       " 'ibm': 8699,\n",
       " '刹': 1172,\n",
       " '##雜': 20486,\n",
       " '惟': 2668,\n",
       " '实': 2141,\n",
       " '艙': 5676,\n",
       " '単': 1299,\n",
       " '##香': 20733,\n",
       " '##呈': 14496,\n",
       " '钿': 7186,\n",
       " '驸': 7726,\n",
       " '##logy': 11121,\n",
       " '鄞': 6969,\n",
       " '丨': 701,\n",
       " '##笔': 18068,\n",
       " '芊': 5691,\n",
       " '銭': 7074,\n",
       " '##蓄': 18955,\n",
       " '怯': 2598,\n",
       " 'dnf': 11315,\n",
       " '##帚': 15424,\n",
       " '##霑': 20512,\n",
       " '℃': 360,\n",
       " '铿': 7217,\n",
       " 'fgo': 11401,\n",
       " '##帐': 15419,\n",
       " '##惆': 15716,\n",
       " '轎': 6754,\n",
       " '繳': 5260,\n",
       " '##愆': 15745,\n",
       " '誨': 6302,\n",
       " '惯': 2679,\n",
       " '石': 4767,\n",
       " '條': 3454,\n",
       " '觅': 6227,\n",
       " '##康': 15491,\n",
       " '##虽': 19063,\n",
       " 'hdmi': 9578,\n",
       " '##带': 15429,\n",
       " '1952': 9241,\n",
       " '粹': 5122,\n",
       " '柩': 3390,\n",
       " '蠍': 6105,\n",
       " '##帰': 15435,\n",
       " '##理': 17472,\n",
       " '##谧': 19530,\n",
       " '##＠': 21088,\n",
       " '規': 6211,\n",
       " '##港': 17006,\n",
       " '##缄': 18403,\n",
       " '重': 7028,\n",
       " '楷': 3514,\n",
       " '諷': 6327,\n",
       " 'moneydj': 11911,\n",
       " '憚': 2733,\n",
       " '龚': 7988,\n",
       " 'ならワークケートへ': 12154,\n",
       " '##购': 19636,\n",
       " '圣': 1760,\n",
       " '##闇': 20351,\n",
       " 'トラックハック': 12766,\n",
       " '##研': 17834,\n",
       " '甄': 4488,\n",
       " '釐': 7031,\n",
       " '##ugh': 12667,\n",
       " 'あります': 12737,\n",
       " 'acer': 10837,\n",
       " '##錚': 20147,\n",
       " '界': 4518,\n",
       " '怏': 2583,\n",
       " '琳': 4432,\n",
       " '鴨': 7861,\n",
       " 'walker': 12651,\n",
       " '辺': 6805,\n",
       " '卧': 1309,\n",
       " '楔': 3503,\n",
       " 'program': 11738,\n",
       " '囟': 1727,\n",
       " '##瞠': 17798,\n",
       " '##达': 19866,\n",
       " '2001': 8285,\n",
       " '庁': 2409,\n",
       " '##轟': 19812,\n",
       " 'plus': 8514,\n",
       " '##rder': 12658,\n",
       " '彈': 2492,\n",
       " '盜': 4671,\n",
       " '偏': 974,\n",
       " '##袍': 19208,\n",
       " '恣': 2613,\n",
       " '歳': 3642,\n",
       " '##丼': 13770,\n",
       " '──': 8297,\n",
       " '##梭': 16517,\n",
       " 'スホンサーサイト': 12314,\n",
       " '燙': 4243,\n",
       " 'imsean': 12087,\n",
       " '##渤': 17000,\n",
       " '胧': 5532,\n",
       " '輯': 6744,\n",
       " 'lock': 9189,\n",
       " '月': 3299,\n",
       " '##タ': 11011,\n",
       " '##痍': 17629,\n",
       " '##魷': 20856,\n",
       " '惚': 2666,\n",
       " '吹': 1430,\n",
       " 'source': 9014,\n",
       " '##嘘': 14713,\n",
       " '##cat': 10869,\n",
       " '##芙': 18753,\n",
       " '闆': 7293,\n",
       " '332': 12308,\n",
       " '差': 2345,\n",
       " 'wu': 10552,\n",
       " '##ca': 8808,\n",
       " '##ᗜ': 13488,\n",
       " '##客': 15202,\n",
       " '##蔷': 18982,\n",
       " '##畳': 17589,\n",
       " '夜': 1915,\n",
       " '##垫': 14864,\n",
       " '##媲': 15116,\n",
       " 'まて': 9642,\n",
       " '##骁': 20789,\n",
       " '##護': 19419,\n",
       " '##key': 9938,\n",
       " '##攙': 16164,\n",
       " '##憬': 15796,\n",
       " '##滲': 17073,\n",
       " '##铐': 20253,\n",
       " '##邂': 19972,\n",
       " '帘': 2366,\n",
       " '##ora': 11455,\n",
       " '##個': 14000,\n",
       " '##潸': 17120,\n",
       " '##熾': 17289,\n",
       " '##侑': 13951,\n",
       " '##φ': 13399,\n",
       " '沥': 3769,\n",
       " '##520': 13241,\n",
       " '錢': 7092,\n",
       " '##瘋': 17654,\n",
       " '##ius': 10392,\n",
       " '賣': 6546,\n",
       " '葱': 5876,\n",
       " 'information': 11237,\n",
       " 'dropbox': 11277,\n",
       " '##(': 13324,\n",
       " '猎': 4333,\n",
       " '##儼': 14092,\n",
       " 't2': 10927,\n",
       " 'bob': 10542,\n",
       " '橄': 3576,\n",
       " 'seven': 12684,\n",
       " '苏': 5722,\n",
       " '剛': 1190,\n",
       " '##tch': 9496,\n",
       " 'mv': 8546,\n",
       " 'henry': 11361,\n",
       " '##厢': 14391,\n",
       " '噔': 1683,\n",
       " '8': 129,\n",
       " '歛': 3630,\n",
       " 'suv': 8540,\n",
       " '楹': 3516,\n",
       " '##[': 13339,\n",
       " '##壺': 14960,\n",
       " '稜': 4929,\n",
       " '蹺': 6705,\n",
       " '##墮': 14933,\n",
       " '##曙': 16339,\n",
       " '##绕': 18369,\n",
       " '##釐': 20088,\n",
       " 'mp3': 8388,\n",
       " '狂': 4312,\n",
       " '聚': 5471,\n",
       " '##鷹': 20934,\n",
       " '##pad': 10730,\n",
       " '##志': 15619,\n",
       " 'す': 548,\n",
       " '鋸': 7085,\n",
       " '##ppy': 12738,\n",
       " '榄': 3520,\n",
       " '給': 5183,\n",
       " '啤': 1566,\n",
       " 'ram': 9270,\n",
       " '##less': 9920,\n",
       " '##vas': 12239,\n",
       " '推': 2972,\n",
       " '##姜': 15059,\n",
       " '##蟀': 19148,\n",
       " '##血': 19174,\n",
       " 'ｒ': 8068,\n",
       " '##襲': 19261,\n",
       " '##豐': 19550,\n",
       " '竿': 5004,\n",
       " '##純': 18212,\n",
       " '劊': 1209,\n",
       " '##酐': 20039,\n",
       " 'ては': 9418,\n",
       " '292': 11542,\n",
       " '[unused74]': 74,\n",
       " '燴': 4251,\n",
       " '欲': 3617,\n",
       " '寸': 2189,\n",
       " '##ber': 8956,\n",
       " '##倫': 14018,\n",
       " '##礫': 17905,\n",
       " '赢': 6617,\n",
       " '铆': 7193,\n",
       " '##揭': 16056,\n",
       " '綢': 5200,\n",
       " '##∕': 13530,\n",
       " '##､': 21106,\n",
       " '嵊': 2314,\n",
       " '汾': 3750,\n",
       " '##ことて': 11178,\n",
       " '搪': 3020,\n",
       " '骯': 7756,\n",
       " '歐': 3627,\n",
       " '[unused92]': 92,\n",
       " '##璨': 17528,\n",
       " '伕': 829,\n",
       " '##熒': 17279,\n",
       " '##婚': 15099,\n",
       " '##崴': 15368,\n",
       " '06': 8116,\n",
       " '整': 3146,\n",
       " '牟': 4283,\n",
       " '1958': 8955,\n",
       " '##燒': 17297,\n",
       " '##訶': 19318,\n",
       " 'posts': 10639,\n",
       " '##俎': 13974,\n",
       " 'star': 9012,\n",
       " '##艇': 18730,\n",
       " 'william': 9979,\n",
       " '##收': 16176,\n",
       " '##嫂': 15121,\n",
       " '確': 4825,\n",
       " '匝': 1268,\n",
       " '##眼': 17763,\n",
       " '##泣': 16855,\n",
       " '赋': 6602,\n",
       " '悽': 2657,\n",
       " '故': 3125,\n",
       " '棠': 3478,\n",
       " '凤': 1128,\n",
       " '##挫': 15976,\n",
       " '##ㄏ': 13713,\n",
       " '##返': 19876,\n",
       " 'tower': 11102,\n",
       " 'psp': 11337,\n",
       " '##企': 13878,\n",
       " '撓': 3055,\n",
       " '##《': 13652,\n",
       " '猕': 4334,\n",
       " 'spf': 12598,\n",
       " '蕴': 5943,\n",
       " 'articles': 12431,\n",
       " '羲': 5414,\n",
       " '魅': 7791,\n",
       " 'には': 8738,\n",
       " '##✦': 13636,\n",
       " '##post': 11719,\n",
       " '##隆': 20441,\n",
       " '##ェ': 12690,\n",
       " '##new': 10654,\n",
       " '67': 8369,\n",
       " '##枯': 16426,\n",
       " '迩': 6831,\n",
       " '##吠': 14470,\n",
       " '##﹣': 21069,\n",
       " '1010': 13266,\n",
       " '##ν': 13391,\n",
       " '##廓': 15501,\n",
       " '##湳': 17024,\n",
       " '仁': 785,\n",
       " '棵': 3484,\n",
       " '##ж': 13407,\n",
       " '##谆': 19504,\n",
       " '灶': 4131,\n",
       " '520': 9038,\n",
       " '##雏': 20479,\n",
       " '抖': 2833,\n",
       " '獠': 4358,\n",
       " 'のないフロクに': 10900,\n",
       " 'a5': 10796,\n",
       " '式': 2466,\n",
       " '##浣': 16911,\n",
       " '摸': 3043,\n",
       " '謾': 6347,\n",
       " 'tai': 13242,\n",
       " '##乞': 13794,\n",
       " '##頜': 20584,\n",
       " '[unused93]': 93,\n",
       " '##雅': 20471,\n",
       " '##這': 19914,\n",
       " '##肛': 18554,\n",
       " '##俑': 13977,\n",
       " 'nature': 10467,\n",
       " 'scrum': 12527,\n",
       " '旗': 3186,\n",
       " '##✈': 13632,\n",
       " '啫': 1571,\n",
       " 's3': 10647,\n",
       " '##猥': 17398,\n",
       " '##琉': 17474,\n",
       " '舆': 5644,\n",
       " '鐳': 7135,\n",
       " '##障': 20454,\n",
       " '##nis': 12334,\n",
       " '兀': 1037,\n",
       " '##惯': 15736,\n",
       " '脘': 5557,\n",
       " 'product': 11691,\n",
       " '##貽': 19586,\n",
       " '##嶙': 15383,\n",
       " '轍': 6753,\n",
       " '扶': 2820,\n",
       " '##蕈': 18989,\n",
       " '##轎': 19811,\n",
       " '鋼': 7086,\n",
       " '竇': 4987,\n",
       " '##骛': 20804,\n",
       " '##ond': 11684,\n",
       " '摳': 3042,\n",
       " '嬤': 2085,\n",
       " '氯': 3714,\n",
       " '2cm': 12988,\n",
       " 'value': 10850,\n",
       " 'window': 12158,\n",
       " '##億': 14080,\n",
       " 'lofter': 8117,\n",
       " '＿': 8049,\n",
       " '##惇': 15717,\n",
       " '俎': 917,\n",
       " '榨': 3529,\n",
       " 'uber': 8624,\n",
       " '##算': 18107,\n",
       " '##続': 18254,\n",
       " '##羽': 18474,\n",
       " '##莊': 18857,\n",
       " '欢': 3614,\n",
       " '惜': 2667,\n",
       " '痈': 4569,\n",
       " '盔': 4666,\n",
       " '##鶴': 20931,\n",
       " '##幌': 15446,\n",
       " '2mm': 12287,\n",
       " '艺': 5686,\n",
       " '##荷': 18849,\n",
       " '337': 12129,\n",
       " '##xx': 9517,\n",
       " '56': 8259,\n",
       " 'fifa': 12130,\n",
       " '##袂': 19203,\n",
       " 'wikia': 8708,\n",
       " 'なし': 9930,\n",
       " '肛': 5497,\n",
       " '幫': 2396,\n",
       " '庄': 2411,\n",
       " '##baru': 13257,\n",
       " '##谭': 19535,\n",
       " '揭': 2999,\n",
       " '泉': 3787,\n",
       " '##凉': 14174,\n",
       " '##音': 20566,\n",
       " '##ten': 11598,\n",
       " '桎': 3431,\n",
       " '州': 2336,\n",
       " 'ο': 222,\n",
       " '暖': 3265,\n",
       " '浯': 3860,\n",
       " '覧': 6216,\n",
       " '##joy': 10284,\n",
       " 'tokyo': 10357,\n",
       " '1948': 9130,\n",
       " '##ject': 12238,\n",
       " '##fig': 11392,\n",
       " 'gov': 9514,\n",
       " '桜': 3436,\n",
       " '爆': 4255,\n",
       " '##出': 14196,\n",
       " '槃': 3538,\n",
       " '##己': 15403,\n",
       " '##帽': 15441,\n",
       " '##磁': 17885,\n",
       " '##吵': 14484,\n",
       " '禁': 4881,\n",
       " '軼': 6730,\n",
       " '##ⓘ': 13577,\n",
       " '怡': 2592,\n",
       " '嬪': 2086,\n",
       " '讶': 6385,\n",
       " 'target': 11926,\n",
       " '##喳': 14667,\n",
       " '##薇': 19005,\n",
       " '##123': 9807,\n",
       " '氏': 3694,\n",
       " '误': 6428,\n",
       " '##孪': 15169,\n",
       " '##孕': 15154,\n",
       " '##ﾗ': 21117,\n",
       " '##凸': 14194,\n",
       " '校': 3413,\n",
       " '##跃': 19702,\n",
       " '##闢': 20361,\n",
       " '##股': 18557,\n",
       " '囿': 1746,\n",
       " 'field': 12594,\n",
       " '##嫌': 15123,\n",
       " '##佈': 13911,\n",
       " '##伉': 13879,\n",
       " '##巅': 15387,\n",
       " '##ews': 13024,\n",
       " '##弊': 15521,\n",
       " '##ost': 12085,\n",
       " '##歳': 16699,\n",
       " '虚': 5994,\n",
       " '##てきます': 11088,\n",
       " '詐': 6266,\n",
       " '##mann': 10750,\n",
       " '廉': 2442,\n",
       " '従': 2531,\n",
       " '）': 8021,\n",
       " '##bia': 12596,\n",
       " '##毂': 16731,\n",
       " '##cc': 8860,\n",
       " 'eb': 10979,\n",
       " '刻': 1174,\n",
       " 'gopro': 13176,\n",
       " '##吊': 14453,\n",
       " '##饴': 20712,\n",
       " '##漠': 17087,\n",
       " '嗇': 1620,\n",
       " 'friends': 11488,\n",
       " '##嫣': 15130,\n",
       " '##佟': 13928,\n",
       " '##墊': 14922,\n",
       " '##韻': 20569,\n",
       " '##臃': 18677,\n",
       " '谒': 6457,\n",
       " '抽': 2853,\n",
       " '谪': 6476,\n",
       " 'day1': 11813,\n",
       " '##鸚': 20936,\n",
       " 'powered': 10103,\n",
       " '揚': 2993,\n",
       " 'ck': 11623,\n",
       " '请': 6435,\n",
       " '008': 12210,\n",
       " '##汁': 16780,\n",
       " '効': 1230,\n",
       " '##沱': 16833,\n",
       " '##碳': 17880,\n",
       " '##靳': 20542,\n",
       " '瑪': 4454,\n",
       " '掏': 2959,\n",
       " 'lp': 11195,\n",
       " 'rar': 12773,\n",
       " '冰': 1102,\n",
       " '##lton': 10377,\n",
       " '祟': 4869,\n",
       " 'ｑ': 8067,\n",
       " '玫': 4382,\n",
       " '23': 8133,\n",
       " '##損': 16067,\n",
       " '##癫': 17683,\n",
       " '僮': 1016,\n",
       " '##凹': 14195,\n",
       " '##炽': 17217,\n",
       " '來': 889,\n",
       " '甭': 4504,\n",
       " '##萁': 18898,\n",
       " '##不': 13736,\n",
       " '##ρ': 13394,\n",
       " '##想': 15739,\n",
       " '弄': 2462,\n",
       " '##&': 13322,\n",
       " 'ul': 10293,\n",
       " '俗': 921,\n",
       " '铮': 7209,\n",
       " '##覺': 19278,\n",
       " '##捉': 15986,\n",
       " '恶': 2626,\n",
       " '痔': 4574,\n",
       " '眾': 4707,\n",
       " '101': 8359,\n",
       " '烷': 4180,\n",
       " '苁': 5717,\n",
       " '##蚱': 19079,\n",
       " '夭': 1924,\n",
       " '415': 12114,\n",
       " '##枸': 16432,\n",
       " '吶': 1428,\n",
       " '##主': 13769,\n",
       " '##丞': 13750,\n",
       " '##牍': 17335,\n",
       " '龜': 7990,\n",
       " '##・': 13703,\n",
       " '鋤': 7082,\n",
       " '##腴': 18647,\n",
       " '叙': 1360,\n",
       " '##舜': 18715,\n",
       " '##啄': 14611,\n",
       " '権': 3565,\n",
       " '##丈': 13732,\n",
       " '10℃': 9115,\n",
       " '##鄱': 20031,\n",
       " '坷': 1794,\n",
       " '##汩': 16798,\n",
       " '闾': 7321,\n",
       " '薰': 5962,\n",
       " '辨': 6795,\n",
       " '##卡': 14362,\n",
       " '呲': 1455,\n",
       " '##gence': 12932,\n",
       " '##皰': 17707,\n",
       " '##晦': 16303,\n",
       " '贓': 6561,\n",
       " '測': 3947,\n",
       " 'open': 8893,\n",
       " '困': 1737,\n",
       " '底': 2419,\n",
       " '疤': 4552,\n",
       " 'xyz': 11266,\n",
       " '##kins': 13084,\n",
       " '##催': 14055,\n",
       " '##昊': 16264,\n",
       " '言': 6241,\n",
       " '##索': 18221,\n",
       " '臣': 5628,\n",
       " '腔': 5579,\n",
       " '##├': 13584,\n",
       " 'mbc': 10648,\n",
       " 'images': 12804,\n",
       " '229': 10608,\n",
       " '##僑': 14066,\n",
       " '##啶': 14635,\n",
       " '##讶': 19442,\n",
       " '##靈': 20527,\n",
       " '祯': 4875,\n",
       " '##mit': 11104,\n",
       " '2100': 10247,\n",
       " 'marc': 11511,\n",
       " '##tina': 10584,\n",
       " '☆☆☆': 12201,\n",
       " '##符': 18073,\n",
       " '颌': 7571,\n",
       " '##袅': 19205,\n",
       " '##23': 8748,\n",
       " '##缸': 18431,\n",
       " '攒': 3104,\n",
       " '▌': 456,\n",
       " '紮': 5167,\n",
       " '##疑': 17599,\n",
       " '皆': 4639,\n",
       " '##磨': 17893,\n",
       " '##娜': 15082,\n",
       " '##侥': 13959,\n",
       " '您': 2644,\n",
       " '徙': 2535,\n",
       " '##承': 15881,\n",
       " '##夷': 14986,\n",
       " '##嵇': 15370,\n",
       " 'windows': 8235,\n",
       " '粵': 5121,\n",
       " '艦': 5677,\n",
       " 'raid': 11044,\n",
       " '雕': 7425,\n",
       " '商': 1555,\n",
       " '⒈': 420,\n",
       " '篷': 5076,\n",
       " 'by': 8120,\n",
       " '頁': 7514,\n",
       " '##een': 10929,\n",
       " '宴': 2155,\n",
       " '##red': 9749,\n",
       " '##ndi': 11874,\n",
       " '吨': 1417,\n",
       " '8591': 12376,\n",
       " 'when': 11611,\n",
       " '##叵': 14439,\n",
       " '缓': 5353,\n",
       " '##65': 9331,\n",
       " '##tory': 12608,\n",
       " '##娅': 15074,\n",
       " '##寞': 15231,\n",
       " 'feel': 11297,\n",
       " '##秭': 17972,\n",
       " 'с': 249,\n",
       " '密': 2166,\n",
       " '##鳝': 20907,\n",
       " '树': 3409,\n",
       " '盆': 4658,\n",
       " '##尬': 15274,\n",
       " '乐': 727,\n",
       " '##嘰': 14727,\n",
       " '绞': 5319,\n",
       " '貸': 6526,\n",
       " '##巢': 15395,\n",
       " '竄': 4985,\n",
       " '155': 9437,\n",
       " '##布': 15414,\n",
       " '183': 9826,\n",
       " '##ː': 13376,\n",
       " '##褫': 19250,\n",
       " '注': 3800,\n",
       " '##感': 15754,\n",
       " '紙': 5158,\n",
       " '##呼': 14518,\n",
       " '##觅': 19284,\n",
       " '##‿': 13509,\n",
       " '托': 2805,\n",
       " 'メーカー': 12052,\n",
       " '##vy': 11853,\n",
       " '##urg': 13143,\n",
       " '##劍': 14267,\n",
       " '##敝': 16195,\n",
       " '發': 4634,\n",
       " '##ach': 10289,\n",
       " '##↑': 10839,\n",
       " 'sofascore': 11555,\n",
       " 'am09': 12377,\n",
       " '##皑': 17701,\n",
       " '##隅': 20440,\n",
       " 'atom': 8941,\n",
       " '匾': 1279,\n",
       " '##un': 8829,\n",
       " '##煖': 17262,\n",
       " '佑': 859,\n",
       " '##芦': 18758,\n",
       " '##吳': 14482,\n",
       " '##记': 19438,\n",
       " 'course': 12654,\n",
       " '疝': 4548,\n",
       " '茉': 5748,\n",
       " '止': 3632,\n",
       " '780': 11493,\n",
       " '梗': 3453,\n",
       " '犁': 4298,\n",
       " '讽': 6391,\n",
       " 'part': 9124,\n",
       " 'reuters': 11778,\n",
       " '##墜': 14928,\n",
       " '##斂': 16207,\n",
       " '##短': 17821,\n",
       " '疵': 4560,\n",
       " 'く': 543,\n",
       " '##狰': 17384,\n",
       " '##箫': 18111,\n",
       " '##se': 8417,\n",
       " '##佳': 13938,\n",
       " '##积': 17973,\n",
       " '╭': 443,\n",
       " '茫': 5755,\n",
       " '収': 1354,\n",
       " '##麵': 20991,\n",
       " '257': 11027,\n",
       " '巡': 2337,\n",
       " '##鰻': 20873,\n",
       " 'cdma': 12902,\n",
       " '1943': 9540,\n",
       " '##腫': 18641,\n",
       " '##蔣': 18976,\n",
       " 'vogue': 10137,\n",
       " '授': 2956,\n",
       " '##绽': 18399,\n",
       " '##刽': 14232,\n",
       " '##oto': 11563,\n",
       " 'joseph': 11151,\n",
       " '##貼': 19585,\n",
       " 'rainer': 13076,\n",
       " '##咗': 14534,\n",
       " 'い': 536,\n",
       " '守': 2127,\n",
       " '唠': 1541,\n",
       " '締': 5224,\n",
       " '鼾': 7966,\n",
       " '##冬': 14157,\n",
       " '##圈': 14807,\n",
       " '范': 5745,\n",
       " '##览': 19286,\n",
       " 'docomo': 11358,\n",
       " '给': 5314,\n",
       " '##吩': 14475,\n",
       " '籬': 5098,\n",
       " '##∮': 13538,\n",
       " '##别': 14223,\n",
       " '##环': 17441,\n",
       " 'off': 9594,\n",
       " 'dt': 13260,\n",
       " '##垃': 14853,\n",
       " '撚': 3057,\n",
       " '赏': 6605,\n",
       " '##苏': 18779,\n",
       " '201': 9250,\n",
       " '##螞': 19142,\n",
       " '忍': 2556,\n",
       " '##脖': 18613,\n",
       " 'photos': 9999,\n",
       " '1867': 13042,\n",
       " 'pdf': 8496,\n",
       " '酊': 6978,\n",
       " '噤': 1689,\n",
       " '舔': 5654,\n",
       " '础': 4794,\n",
       " '64gb': 11432,\n",
       " '##滿': 17078,\n",
       " '##局': 15286,\n",
       " '哇': 1505,\n",
       " '##鞣': 20553,\n",
       " '竭': 4998,\n",
       " '##輩': 19799,\n",
       " 'address': 13093,\n",
       " '宸': 2158,\n",
       " 'get': 9018,\n",
       " '姉': 1991,\n",
       " '##n': 8171,\n",
       " 'oo': 11383,\n",
       " '淀': 3895,\n",
       " '墩': 1875,\n",
       " '20g': 12793,\n",
       " '##ない': 8560,\n",
       " '##sday': 12956,\n",
       " 'hotel': 8462,\n",
       " '闫': 7307,\n",
       " '这': 6821,\n",
       " '##銬': 20130,\n",
       " '串': 706,\n",
       " '##疔': 17600,\n",
       " '鍵': 7107,\n",
       " '缇': 5349,\n",
       " 'lu': 12015,\n",
       " '[unused41]': 41,\n",
       " '挺': 2923,\n",
       " '炔': 4144,\n",
       " '緣': 5225,\n",
       " '場': 1842,\n",
       " '箴': 5057,\n",
       " '##赴': 19683,\n",
       " 'ｃｐ': 8836,\n",
       " '捷': 2949,\n",
       " '淄': 3896,\n",
       " '針': 7036,\n",
       " 'dv': 12372,\n",
       " '##ision': 12556,\n",
       " '##また': 13150,\n",
       " '撂': 3047,\n",
       " '蜗': 6054,\n",
       " '啵': 1577,\n",
       " '踊': 6670,\n",
       " '權': 3609,\n",
       " '##灸': 17189,\n",
       " 'kiss': 11192,\n",
       " '##药': 18847,\n",
       " '##彌': 15550,\n",
       " '簦': 5081,\n",
       " '##⑨': 13564,\n",
       " '君': 1409,\n",
       " '结': 5310,\n",
       " '1960': 8779,\n",
       " '豆': 6486,\n",
       " '楠': 3507,\n",
       " '##犸': 17366,\n",
       " '6a': 11692,\n",
       " 'victoria': 12445,\n",
       " '##炜': 17205,\n",
       " 'zenfone': 11726,\n",
       " '##賺': 19610,\n",
       " '##鸣': 20942,\n",
       " '##惕': 15721,\n",
       " '##列': 14211,\n",
       " '嵌': 2316,\n",
       " '罢': 5387,\n",
       " 'thread': 11164,\n",
       " '##tel': 11246,\n",
       " '閱': 7288,\n",
       " 'maps': 12446,\n",
       " '鑰': 7145,\n",
       " '##叢': 14422,\n",
       " '○○': 9423,\n",
       " '625': 12218,\n",
       " 'life': 8562,\n",
       " '##啾': 14640,\n",
       " '##泷': 16866,\n",
       " '##粥': 18171,\n",
       " '厭': 1339,\n",
       " 'pet': 10495,\n",
       " '##帝': 15427,\n",
       " '##go': 8591,\n",
       " '##亥': 13827,\n",
       " '##潜': 17109,\n",
       " 'systems': 12451,\n",
       " '407': 12458,\n",
       " '捺': 2950,\n",
       " '##城': 14871,\n",
       " '##銅': 20124,\n",
       " '##ᵃ': 13489,\n",
       " '418': 12978,\n",
       " '##walk': 12621,\n",
       " '##恢': 15669,\n",
       " '##旎': 16240,\n",
       " 'sean': 12794,\n",
       " '##柳': 16451,\n",
       " '##鍋': 20159,\n",
       " '▽': 469,\n",
       " '悯': 2648,\n",
       " 'iu': 13172,\n",
       " '##ュ': 13697,\n",
       " '##弁': 15516,\n",
       " '逶': 6870,\n",
       " '##释': 20082,\n",
       " 'villa': 10806,\n",
       " 'louis': 10950,\n",
       " '嘸': 1675,\n",
       " '羞': 5404,\n",
       " 'g1': 11528,\n",
       " '嚣': 1709,\n",
       " '键': 7241,\n",
       " '黝': 7952,\n",
       " '##剖': 14246,\n",
       " '##鵰': 20926,\n",
       " '##～20': 12078,\n",
       " '乩': 742,\n",
       " '##黍': 21000,\n",
       " '##镉': 20310,\n",
       " '祁': 4854,\n",
       " '珣': 4404,\n",
       " 'む': 569,\n",
       " '##杓': 16391,\n",
       " '彼': 2516,\n",
       " '腊': 5572,\n",
       " '譜': 6355,\n",
       " '1992': 8508,\n",
       " '##僅': 14063,\n",
       " '##左': 15397,\n",
       " '侧': 904,\n",
       " '##棕': 16530,\n",
       " '##椰': 16553,\n",
       " '##砗': 17836,\n",
       " '濱': 4097,\n",
       " '##ery': 11041,\n",
       " '纺': 5293,\n",
       " 'bin': 11702,\n",
       " '##辺': 19862,\n",
       " '##郜': 20006,\n",
       " 'ui': 8840,\n",
       " '脏': 5552,\n",
       " 'f16': 12799,\n",
       " '##鹜': 20966,\n",
       " '##栈': 16461,\n",
       " 'live': 8582,\n",
       " '##滇': 17052,\n",
       " '昨': 3219,\n",
       " '櫻': 3607,\n",
       " 'long': 10037,\n",
       " '瀝': 4110,\n",
       " '1959': 9110,\n",
       " '##ald': 13041,\n",
       " '##馗': 20732,\n",
       " '漏': 4026,\n",
       " '佃': 851,\n",
       " '##ark': 11153,\n",
       " '莲': 5813,\n",
       " '##芾': 18773,\n",
       " 'shopping': 9662,\n",
       " '##儀': 14078,\n",
       " '恚': 2611,\n",
       " '璐': 4466,\n",
       " '蜆': 6046,\n",
       " '遵': 6905,\n",
       " '韓': 7502,\n",
       " 'category': 10633,\n",
       " '##宝': 15197,\n",
       " 'forum': 11730,\n",
       " '##篮': 18131,\n",
       " 'namespace': 11076,\n",
       " '异': 2460,\n",
       " '壅': 1881,\n",
       " '##麂': 20980,\n",
       " '驹': 7727,\n",
       " '##麩': 20989,\n",
       " '裂': 6162,\n",
       " '##亡': 13824,\n",
       " '鹵': 7918,\n",
       " 'online': 8314,\n",
       " '##per': 9063,\n",
       " 'over': 10047,\n",
       " 'daniel': 9701,\n",
       " '##いました': 11350,\n",
       " '途': 6854,\n",
       " '##玥': 17437,\n",
       " '鱔': 7820,\n",
       " '裳': 6178,\n",
       " '##2008': 10545,\n",
       " '##标': 16460,\n",
       " '100': 8135,\n",
       " '阑': 7332,\n",
       " '##gb': 9673,\n",
       " '仇': 790,\n",
       " '振': 2920,\n",
       " '##鳍': 20903,\n",
       " '昶': 3225,\n",
       " '鹧': 7913,\n",
       " '袭': 6159,\n",
       " '麓': 7926,\n",
       " '##ˢ': 13379,\n",
       " '##ب': 13428,\n",
       " '添': 3924,\n",
       " '骈': 7738,\n",
       " '愆': 2688,\n",
       " 'usd': 9259,\n",
       " '～1': 12756,\n",
       " '##drive': 13115,\n",
       " '##▇': 13600,\n",
       " '41': 8245,\n",
       " '##铬': 20264,\n",
       " '##ᄂ': 13456,\n",
       " '##龜': 21047,\n",
       " '淪': 3914,\n",
       " '900': 8567,\n",
       " '##ren': 9489,\n",
       " 'ｈ': 8058,\n",
       " '##pin': 13248,\n",
       " '##安': 15185,\n",
       " '##沟': 16822,\n",
       " '##萦': 18910,\n",
       " '##蛀': 19081,\n",
       " 'ssd': 9004,\n",
       " '##衔': 19181,\n",
       " '##析': 16415,\n",
       " '##鍰': 20163,\n",
       " '瀏': 4104,\n",
       " '##瑚': 17502,\n",
       " '洽': 3835,\n",
       " '镕': 7258,\n",
       " '⑧': 412,\n",
       " '摒': 3034,\n",
       " '驕': 7709,\n",
       " '鸪': 7889,\n",
       " '樾': 3575,\n",
       " '000': 8241,\n",
       " '裙': 6170,\n",
       " '裹': 6181,\n",
       " '##ads': 12514,\n",
       " '##燦': 17304,\n",
       " '##促': 13971,\n",
       " '##厕': 14386,\n",
       " '##cs': 9761,\n",
       " '四': 1724,\n",
       " '鏡': 7128,\n",
       " '##梆': 16507,\n",
       " '##錶': 20157,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab  # 有些有两个##表示在做一些子词的处理，可以缩小词表，很多词可以用目前的词组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换成词典对应的id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以将id序列转换成词序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以将tokens转换成句子\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更便捷的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串直接转换成分词id，而不需要分词之后再转换成id序列\n",
    "ids = tokenizer.encode(sen)  # 这种编码方式会在整个句子的开头加上开始符号cls，在句子的末尾加上末尾符号sep\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果不想看到特俗符号的话，那么就可以添加一个add_special_tokens设置为false；就是不添加特俗符号\n",
    "ids = tokenizer.encode(sen, add_special_tokens=False)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode用来解码，将序列解码成原始句子\n",
    "str_sen = tokenizer.decode(ids)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样也有一个skip_special_token参数来告诉你要不要跳过这个special token\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 所有的数据要填充或者截断成统一长度的字符\n",
    "# 填充 padding\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当你的数据存在填充，那你需要告诉模型哪些部分是填充，哪些部分是有效的输入，这个时候就需要一个attention_mask\n",
    "ids = tokenizer.encode(sen, padding='max_length', max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]  # 判断哪些是填充，哪些不是填充\n",
    "token_type_ids = [0] * len(ids)  # 判断是第几个句子的词语\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step7 快速的调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode_plus自动帮你解决分词，转换序列，填充（截断），attention_mask,token_type_ids\n",
    "inputs = tokenizer.encode_plus(sen, padding='max_length', max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding='max_length', max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step8 批处理一批数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 3173, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对多条数据的处理,这种的处理方式更快\n",
    "sens = [\"弱小的我也有大梦想!\", \"有梦想谁都了不起\", \"追逐梦想的新，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 172 ms\n",
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 172 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# batch处理，这种方式更快\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast / Slow  Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认使用的是快速的tokenizer,能支持fast的建议使用fast\n",
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 慢的tokenizer,实现的速度比较慢\n",
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 484 ms\n",
      "Wall time: 515 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条的循环处理\n",
    "for i in range(1000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 891 ms\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条的循环处理\n",
    "for i in range(1000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# batch处理\n",
    "res = fast_tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 750 ms\n",
      "Wall time: 850 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# batch处理\n",
    "res = slow_tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()  # 会记录每一个词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特殊的Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 441/441 [00:00<00:00, 147kB/s]\n",
      "C:\\Users\\yj.wang\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yj.wang\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenization_chatglm.py: 100%|██████████| 17.0k/17.0k [00:00<00:00, 5.70MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "ice_text.model: 100%|██████████| 2.71M/2.71M [00:00<00:00, 4.37MB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\论文-学习资料\\code\\TRANSFORMERS\\00-introduction\\02-tokenizer.ipynb Cell 47\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E8%AE%BA%E6%96%87-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/code/TRANSFORMERS/00-introduction/02-tokenizer.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E8%AE%BA%E6%96%87-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/code/TRANSFORMERS/00-introduction/02-tokenizer.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# 有些模型并不是huggingface自己实现的，有些是别人实现的上传到上面去的，因此在使用这些特殊的Tokenizer的时候需要使用到trust_remote_code=True\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/%E8%AE%BA%E6%96%87-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/code/TRANSFORMERS/00-introduction/02-tokenizer.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mTHUDM/chatglm-6b\u001b[39;49m\u001b[39m\"\u001b[39;49m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/%E8%AE%BA%E6%96%87-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/code/TRANSFORMERS/00-introduction/02-tokenizer.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:755\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[0;32m    754\u001b[0m         tokenizer_class\u001b[39m.\u001b[39mregister_for_auto_class()\n\u001b[1;32m--> 755\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    756\u001b[0m \u001b[39melif\u001b[39;00m config_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     tokenizer_class \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2022\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2024\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2025\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2026\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2027\u001b[0m     init_configuration,\n\u001b[0;32m   2028\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   2029\u001b[0m     token\u001b[39m=\u001b[39mtoken,\n\u001b[0;32m   2030\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2031\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2032\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2033\u001b[0m     _is_local\u001b[39m=\u001b[39mis_local,\n\u001b[0;32m   2034\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2035\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2254\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2256\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs)\n\u001b[0;32m   2257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   2258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   2259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2261\u001b[0m     )\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b\\8b7d33596d18c5e83e2da052d05ca4db02e60620\\tokenization_chatglm.py:196\u001b[0m, in \u001b[0;36mChatGLMTokenizer.__init__\u001b[1;34m(self, vocab_file, do_lower_case, remove_space, bos_token, eos_token, end_token, mask_token, gmask_token, padding_side, pad_token, unk_token, num_image_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    181\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    182\u001b[0m         vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    195\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    197\u001b[0m         do_lower_case\u001b[39m=\u001b[39mdo_lower_case,\n\u001b[0;32m    198\u001b[0m         remove_space\u001b[39m=\u001b[39mremove_space,\n\u001b[0;32m    199\u001b[0m         padding_side\u001b[39m=\u001b[39mpadding_side,\n\u001b[0;32m    200\u001b[0m         bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[0;32m    201\u001b[0m         eos_token\u001b[39m=\u001b[39meos_token,\n\u001b[0;32m    202\u001b[0m         end_token\u001b[39m=\u001b[39mend_token,\n\u001b[0;32m    203\u001b[0m         mask_token\u001b[39m=\u001b[39mmask_token,\n\u001b[0;32m    204\u001b[0m         gmask_token\u001b[39m=\u001b[39mgmask_token,\n\u001b[0;32m    205\u001b[0m         pad_token\u001b[39m=\u001b[39mpad_token,\n\u001b[0;32m    206\u001b[0m         unk_token\u001b[39m=\u001b[39munk_token,\n\u001b[0;32m    207\u001b[0m         num_image_tokens\u001b[39m=\u001b[39mnum_image_tokens,\n\u001b[0;32m    208\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[0;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_space \u001b[39m=\u001b[39m remove_space\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils.py:367\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    365\u001b[0m \u001b[39m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_tokens(\n\u001b[0;32m    368\u001b[0m     [token \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens_extended \u001b[39mif\u001b[39;49;00m token \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_added_tokens_encoder],\n\u001b[0;32m    369\u001b[0m     special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils.py:467\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._add_tokens\u001b[1;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m added_tokens\n\u001b[0;32m    466\u001b[0m \u001b[39m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m current_vocab \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vocab()\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    468\u001b[0m new_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(current_vocab)  \u001b[39m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m new_tokens:\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b\\8b7d33596d18c5e83e2da052d05ca4db02e60620\\tokenization_chatglm.py:248\u001b[0m, in \u001b[0;36mChatGLMTokenizer.get_vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns vocab as a dict \"\"\"\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     vocab \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_id_to_token(i): i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_size)}\n\u001b[0;32m    249\u001b[0m     vocab\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder)\n\u001b[0;32m    250\u001b[0m     \u001b[39mreturn\u001b[39;00m vocab\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b\\8b7d33596d18c5e83e2da052d05ca4db02e60620\\tokenization_chatglm.py:244\u001b[0m, in \u001b[0;36mChatGLMTokenizer.vocab_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab_size\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns vocab size \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp_tokenizer\u001b[39m.\u001b[39mnum_tokens\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# 有些模型并不是huggingface自己实现的，有些是别人实现的上传到上面去的，因此在使用这些特殊的Tokenizer的时候需要使用到trust_remote_code=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer\n",
    "# 报错的原因好像是这个模型不支持更新版本的transformer\n",
    "# 参考：https://github.com/THUDM/VisualGLM-6B/issues/333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e13289ac6049fa983906eb8381b46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/268 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yj.wang\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yj.wang\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b18af5ec17d482eb1dfb5024512d6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb366f22366f46bbad397faeab7344d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-389m-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Langboat/bloom-389m-zh\\\\tokenizer_config.json',\n",
       " './Langboat/bloom-389m-zh\\\\special_tokens_map.json',\n",
       " './Langboat/bloom-389m-zh\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./Langboat/bloom-389m-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='./Langboat/bloom-389m-zh', vocab_size=42437, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./Langboat/bloom-389m-zh\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
